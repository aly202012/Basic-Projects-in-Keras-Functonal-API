# -*- coding: utf-8 -*-
"""Combined and assembled models for classification MNIST dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XSPSQ_7Loo_4URj9-p_PqYfRVIQUGDBs
"""

# keras modules
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Activation, Dense, Dropout
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten
from tensorflow.keras.utils import to_categorical, plot_model
from tensorflow.keras.datasets import mnist

"""# load mnist data"""

(x_train, y_train), (x_test, y_test) = mnist.load_data()

print('shape of x_train' , x_train.shape)
print('shape of y_train' , y_train.shape)
print('shape of x_test' , x_test.shape)
print('shape of y_test' , y_test.shape)

"""## Show a data item"""

import matplotlib.pyplot as plt
digit=x_train[8]
plt.imshow(digit, cmap=plt.cm.binary)
plt.show()

"""## Data preprocessing"""

# compute the number of labels
number_labels=len(np.unique(y_train))
number_labels

# convert y_train & y_test to one-hot vector
y_train_2=to_categorical(y_train)
y_test_2=to_categorical(y_test)

print('y_train before : ' ,y_train.shape )
print('y_train after : ' , y_train_2.shape)

"""## Adjust the dimensions to fit the network."""

print('x_train.shape :' , x_train.shape)
image_size=x_train.shape[1]
print('image size : ', image_size)

"""### resizeing & normalization"""

x_train = np.reshape(x_train,[-1, image_size, image_size, 1])
x_test = np.reshape(x_test,[-1, image_size, image_size, 1])
x_train = x_train.astype('float32') / 255
x_test = x_test.astype('float32') / 255

print('x_train after :',x_train.shape)
print('x_test after :' , x_test.shape)

"""## network parameters"""

inputs_shape=(image_size,image_size ,1)
batch_size=128
kernel_size=3
pool_size=2
filters=64
dropout=0.2

"""I will now build a set of mods for the coiled neural network with different structures and then assemble them into one model, then I will work on compiling the results and choosing the best output among them through a statistically significant method.

## The first model is a convolutional neural network.
"""

import tensorflow
from tensorflow import keras
from keras import layers
from tensorflow.keras.utils import  plot_model

def get_model_1():
  inputs=keras.Input(shape=inputs_shape,name='img')
  x=layers.Conv2D(filters=filters,
                        kernel_size=kernel_size,
                        activation='relu')(inputs)
  x=layers.MaxPooling2D(pool_size)(x)
  x=layers.Conv2D(filters=filters,
                        kernel_size=kernel_size,
                        activation='relu')(x)
  x=layers.MaxPooling2D(pool_size)(x)
  x=layers.Conv2D(filters=filters,
                        kernel_size=kernel_size,
                        activation='relu')(x)
  x=layers.Flatten()(x)
  x=layers.Dropout(dropout)(x)
  x=layers.Dense(number_labels)(x)
  output_model_1=layers.Activation('softmax')(x)
  return keras.Model(inputs,output_model_1, name='model_1')

def get_model_2():
  inputs=keras.Input(shape=inputs_shape,name='img')
  x=layers.Conv2D(filters=16,
                        kernel_size=kernel_size,
                        activation='relu')(inputs)
  x=layers.Conv2D(filters=32,
                        kernel_size=kernel_size,
                        activation='relu')(x)
  x=layers.MaxPooling2D(pool_size)(x)
  x=layers.Conv2D(filters=32,
                        kernel_size=kernel_size,
                        activation='relu')(x)
  x=layers.Conv2D(filters=16,
                        kernel_size=kernel_size,
                        activation='relu')(x)
  x=layers.MaxPooling2D(pool_size)(x)
  x=layers.Conv2D(filters=16,
                        kernel_size=kernel_size,
                        activation='relu')(x)
  x=layers.Flatten()(x)
  x=layers.Dropout(0.3)(x)
  x=layers.Dense(number_labels)(x)
  output_model_2=layers.Activation('softmax')(x)
  return keras.Model(inputs,output_model_2, name='model_2')

def get_model_3():
  inputs=keras.Input(shape=inputs_shape,name='img')
  x=layers.Conv2D(filters=32,
                        kernel_size=kernel_size,
                        activation='relu')(inputs)
  x=layers.MaxPooling2D(pool_size)(x)
  x=layers.Conv2D(filters=16,
                        kernel_size=kernel_size,
                        activation='relu')(x)
  x=layers.Flatten()(x)
  x=layers.Dropout(0.05)(x)
  x=layers.Dense(number_labels)(x)
  output_model_3=layers.Activation('softmax')(x)
  return keras.Model(inputs,output_model_3, name='model_3')

"""## Let's start assembling"""

model_1=get_model_1()
model_2=get_model_2()
model_3=get_model_3()

inputs=keras.Input(shape=inputs_shape , name='image')
y_1=model_1(inputs)
y_2=model_2(inputs)
y_3=model_3(inputs)

outputs=layers.average([y_1,y_2,y_3])
ensemble_model=keras.Model(inputs=inputs , outputs=outputs)

ensemble_model.summary()

plot_model(ensemble_model, to_file='ensemble_model.png', show_shapes=True)

model_1.summary()

plot_model(model_1,to_file='model_1.png' ,show_shapes=True,show_dtype=True ,show_layer_activations=True)

model_2.summary()

plot_model(model_2,to_file='model_2.png' ,show_shapes=True,show_dtype=True ,show_layer_activations=True)

model_3.summary()

plot_model(model_3,to_file='model_3.png' ,show_shapes=True,show_dtype=True ,show_layer_activations=True)

"""**Simply put, what happened above, is that I built each model separately from the other with all its parts, except for the training and testing part that I will apply to the assembled model.**

## Now let's complete the compileing and training.
"""

# loss function for one-hot vector
# use of adam optimizer
# accuracy is good metric for classification tasks
ensemble_model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

# train the ensemble model
history=ensemble_model.fit(x_train, y_train_2, epochs=10, batch_size=batch_size)

_, acc = ensemble_model.evaluate(x_test,
                        y_test_2,
                        batch_size=batch_size,
                        verbose=0)
print("\nTest accuracy: %.1f%%" % (100.0 * acc))

"""**It looks like we've reduced the overfitting value from the previous project, which you'll find here:
[Simple_CNN_model_for_classification_MNIST_dataset](https://github.com/aly202012/General-Projects/blob/main/Simple_CNN_model_for_classification_MNIST_dataset.ipynb)
 , In the results, we note that the level of accuracy of training reached 99.1%, while the level of accuracy of the test reached 99.1%, the difference between the two is within parts of a hundred.**
"""

# Print the results on the chart
# Accurasy
plt.plot(history.history['accuracy'])
plt.title('Model Accuracy vs Epoch')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

# tracing the path of loss
plt.plot(history.history['loss'])
plt.title('Model Loss vs Epoch')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper right')
plt.show()

"""## **I will stop now to continue later with new tools and techniques.**"""