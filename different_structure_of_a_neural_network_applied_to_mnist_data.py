# -*- coding: utf-8 -*-
"""Different Structure of a Neural Network applied to MNIST data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XSPSQ_7Loo_4URj9-p_PqYfRVIQUGDBs

## Introduction

In this part, I will review the structure of one of the convolutional neural networks that I will build, and through the structure we will notice the use of many concepts and methods in building neural networks in general.

## Calling the relevant libraries
"""

# keras modules
import numpy as np
from tensorflow.keras.layers import Activation, Dense, Dropout
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.layers import Flatten, concatenate
from tensorflow.keras.utils import to_categorical, plot_model
from tensorflow.keras.datasets import mnist

#import ipyplot
import tensorflow
from tensorflow import keras
from keras import layers , Input 
from tensorflow.keras.models import Model
from tensorflow.keras.utils import  plot_model

"""# loading mnist data"""

(x_train, y_train), (x_test, y_test) = mnist.load_data()

print('shape of x_train' , x_train.shape)
print('shape of y_train' , y_train.shape)
print('shape of x_test' , x_test.shape)
print('shape of y_test' , y_test.shape)

"""## Showing a data item"""

pip install ipyplot

import ipyplot

ipyplot.plot_images(x_train, max_images=5, img_width=150)

#import matplotlib.pyplot as plt
#digit=x_train[8]
#plt.imshow(digit, cmap=plt.cm.binary)
#plt.show()

"""## Data preprocessing"""

# compute the number of labels
number_labels=len(np.unique(y_train))
number_labels

# convert y_train & y_test to one-hot vector
y_train_2=to_categorical(y_train)
y_test_2=to_categorical(y_test)

print('y_train before : ' ,y_train.shape )
print('y_train after : ' , y_train_2.shape)

"""## Adjust the dimensions to fit the network."""

print('x_train.shape :' , x_train.shape)
image_size=x_train.shape[1]
print('image size : ', image_size)

"""### resizeing & normalization"""

x_train = np.reshape(x_train,[-1, image_size, image_size, 1])
x_test = np.reshape(x_test,[-1, image_size, image_size, 1])
x_train = x_train.astype('float32') / 255
x_test = x_test.astype('float32') / 255

print('x_train after :',x_train.shape)
print('x_test after :' , x_test.shape)

"""## network parameters"""

# network parameters
input_shape = (image_size, image_size, 1)
batch_size = 32
kernel_size = 3
dropout = 0.4
n_filters = 32

"""# Building a Neural Network

**The main part of the structure of the neural network that I will build is a neural network with two inputs and one output, and we will rely on the concept of Functional API in Keras**

In order to build a beautiful neural network with two inputs, I must divide the task, which is, build the right part first, then the left part, then collect the parts and merge them into a layer using one of the mergers, then build a special part of the layers with a forked connection in order to overcome the overfitting This part will be inside the Dense layers, and this is good, and therefore I will be satisfied with that for now.

## Right Branch
"""

# right branch of the neural network
right_inputs = Input(shape=input_shape)
y = right_inputs
filters = n_filters
# 3 layers of Conv2D-Dropout-MaxPooling2D
# number of filters doubles after each layer (32-64-128)
for i in range(3):
    y = Conv2D(filters=filters,
               kernel_size=kernel_size,
               padding='same',
               activation='relu',
               dilation_rate=2)(y)
    y = Dropout(dropout)(y)
    y = MaxPooling2D()(y)
    filters *= 2

"""## Left Branch"""

# left branch of neural network
left_inputs = Input(shape=input_shape)
x = left_inputs
filters = n_filters
# 3 layers of Conv2D-Dropout-MaxPooling2D
# number of filters doubles after each layer (32-64-128)
for i in range(3):
    x = Conv2D(filters=filters,
               kernel_size=kernel_size,
               padding='same',
               activation='relu')(x)
    x = Dropout(dropout)(x)
    x = MaxPooling2D()(x)
    filters *= 2

"""## Merge the left and right branches together in one layer."""

# merge left and right branches outputs
y = concatenate([x, y])

"""## Complete neural network structure.

**Through this part of the network I will overcome the part of the overfitting that I encountered in many of the previous structures, let's wait and see the results.**
"""

# feature maps to vector before connecting to Dense 
y=Flatten()(y)
first_dense=Dense(y.shape[1], activation='relu')(y)
y=Dropout(dropout)(first_dense)
y=Dense(y.shape[1],activation='relu')(y)
y=Dropout(0.3)(y)
y=layers.add([first_dense,y])
output=Dense(number_labels,activation='softmax')(y)

# build the model in functional API
model = Model([left_inputs, right_inputs], output)

"""## Showing the trainable parameters in the network."""

# verify the model using layer text description
model.summary()

plot_model(model, to_file='model.png', show_shapes=True)

"""## Preparing the model for training."""

# classifier loss, Adam optimizer, classifier accuracy
model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

# train the model with input images and labels
model.fit([x_train, x_train],
          y_train_2, 
          validation_data=([x_test, x_test], y_test_2),
          epochs=11,
          batch_size=batch_size)

"""# saveing the results"""

model.save('Different structure of a neural network_model.h5')

"""## Evaluation"""

# model accuracy on test dataset
score = model.evaluate([x_test, x_test],
                       y_test_2,
                       batch_size=batch_size,
                       verbose=0)
print("\nTest accuracy: %.1f%%" % (100.0 * score[1]))

"""## I've completely overcome the overfitting problem, and that's pretty good.

**I have been able to overcome the accuracy level of the last neural network structures in the previous projects, and the results are summarized in the following:
The accuracy level of the training data was 98.7%, while the accuracy level of the test data was 99.3%, and the difference between the numbers gives a high indication of overcoming the problem of overfitting.**

## I'll stop now, and continue later.
"""